#!/usr/bin/env python3
"""
å“è³ªè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
Final Boss ãŒBossæˆæœç‰©ã®å“è³ªã‚’è©•ä¾¡ã—ã€çµ±åˆåˆ¤å®šã‚’è¡Œã†
"""

import os
import sys
import json
import subprocess
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from datetime import datetime

class TaskQualityEvaluator:
    """ä»•äº‹å˜ä½ã®å“è³ªè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, org_name: str, task_name: str):
        self.org_name = org_name
        self.task_name = task_name
        self.task_path = Path(f"orgs/{org_name}/{task_name}")
        self.evaluation_results = {}
        
        if not self.task_path.exists():
            raise FileNotFoundError(f"Task directory not found: {self.task_path}")
    
    def evaluate_completion(self) -> Dict:
        """å®Œäº†å“è³ªã®ç·åˆè©•ä¾¡"""
        print(f"ğŸ” {self.org_name} {self.task_name} å“è³ªè©•ä¾¡é–‹å§‹...")
        
        results = {
            'functional_test': self._check_functional_requirements(),
            'code_quality': self._check_code_quality(),
            'test_coverage': self._check_test_coverage(),
            'documentation': self._check_documentation(),
            'performance': self._check_performance(),
        }
        
        overall_score = self._calculate_overall_score(results)
        judgment = self._make_integration_judgment(overall_score, results)
        
        evaluation_report = {
            'timestamp': datetime.now().isoformat(),
            'org_name': self.org_name,
            'task_name': self.task_name,
            'overall_score': overall_score,
            'detailed_results': results,
            'judgment': judgment,
            'recommended_action': self._get_recommended_action(judgment),
            'issues': self._extract_issues(results),
            'fix_suggestions': self._generate_fix_suggestions(results)
        }
        
        self._save_evaluation_report(evaluation_report)
        return evaluation_report
    
    def _check_functional_requirements(self) -> Dict:
        """æ©Ÿèƒ½è¦ä»¶ãƒã‚§ãƒƒã‚¯"""
        integrated_path = self.task_path / "integrated"
        
        if not integrated_path.exists():
            return {
                'passed': False,
                'score': 0,
                'details': 'integrated ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“'
            }
        
        # åŸºæœ¬æ§‹é€ ç¢ºèª
        src_exists = (integrated_path / "src").exists()
        tests_exists = (integrated_path / "tests").exists()
        docs_exists = (integrated_path / "docs").exists()
        
        structure_score = sum([src_exists, tests_exists, docs_exists]) * 10
        
        # å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        src_files = list((integrated_path / "src").rglob("*.py")) if src_exists else []
        implementation_score = min(len(src_files) * 10, 30)
        
        # åŸºæœ¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ
        import_test_passed = False
        if src_files:
            try:
                # æœ€åˆã®Pythonãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ
                first_file = src_files[0]
                module_name = first_file.stem
                result = subprocess.run([
                    sys.executable, "-c", 
                    f"import sys; sys.path.append('{integrated_path}/src'); import {module_name}; print('Import OK')"
                ], capture_output=True, text=True, cwd=integrated_path)
                import_test_passed = result.returncode == 0
            except Exception:
                import_test_passed = False
        
        import_score = 30 if import_test_passed else 0
        total_score = structure_score + implementation_score + import_score
        
        return {
            'passed': total_score >= 60,
            'score': total_score,
            'details': {
                'structure_check': f"{'âœ…' if structure_score >= 20 else 'âŒ'} ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€  ({structure_score}/30)",
                'implementation_check': f"{'âœ…' if implementation_score >= 20 else 'âŒ'} å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ« ({implementation_score}/30)",
                'import_check': f"{'âœ…' if import_test_passed else 'âŒ'} ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ ({import_score}/30)",
                'src_files_count': len(src_files),
                'structure_score': structure_score,
                'implementation_score': implementation_score,
                'import_score': import_score
            }
        }
    
    def _check_code_quality(self) -> Dict:
        """ã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯"""
        integrated_path = self.task_path / "integrated" / "src"
        
        if not integrated_path.exists():
            return {
                'passed': False,
                'score': 0,
                'details': 'src ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“'
            }
        
        # flake8 ãƒã‚§ãƒƒã‚¯
        flake8_result = subprocess.run([
            "flake8", str(integrated_path)
        ], capture_output=True, text=True)
        flake8_passed = flake8_result.returncode == 0
        flake8_score = 40 if flake8_passed else 0
        
        # black ãƒã‚§ãƒƒã‚¯
        black_result = subprocess.run([
            "black", "--check", str(integrated_path)
        ], capture_output=True, text=True)
        black_passed = black_result.returncode == 0
        black_score = 30 if black_passed else 0
        
        # mypy ãƒã‚§ãƒƒã‚¯ (optional)
        mypy_result = subprocess.run([
            "mypy", str(integrated_path)
        ], capture_output=True, text=True)
        mypy_passed = mypy_result.returncode == 0
        mypy_score = 30 if mypy_passed else 15  # éƒ¨åˆ†ç‚¹ã‚ã‚Š
        
        total_score = flake8_score + black_score + mypy_score
        
        return {
            'passed': total_score >= 70,
            'score': total_score,
            'details': {
                'flake8': f"{'âœ…' if flake8_passed else 'âŒ'} flake8 ({flake8_score}/40)",
                'black': f"{'âœ…' if black_passed else 'âŒ'} black formatting ({black_score}/30)",
                'mypy': f"{'âœ…' if mypy_passed else 'âš ï¸'} mypy typing ({mypy_score}/30)",
                'flake8_output': flake8_result.stdout + flake8_result.stderr,
                'black_output': black_result.stdout + black_result.stderr,
                'mypy_output': mypy_result.stdout + mypy_result.stderr
            }
        }
    
    def _check_test_coverage(self) -> Dict:
        """ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ãƒã‚§ãƒƒã‚¯"""
        integrated_path = self.task_path / "integrated"
        
        if not (integrated_path / "tests").exists():
            return {
                'passed': False,
                'score': 0,
                'details': 'tests ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“'
            }
        
        # pytest å®Ÿè¡Œ
        pytest_result = subprocess.run([
            "python", "-m", "pytest", "tests/", "-v", "--tb=short"
        ], capture_output=True, text=True, cwd=integrated_path)
        pytest_passed = pytest_result.returncode == 0
        pytest_score = 50 if pytest_passed else 0
        
        # ã‚«ãƒãƒ¬ãƒƒã‚¸æ¸¬å®š
        coverage_result = subprocess.run([
            "python", "-m", "pytest", "tests/", 
            "--cov=src", "--cov-report=term-missing"
        ], capture_output=True, text=True, cwd=integrated_path)
        
        coverage_percent = 0
        if coverage_result.returncode == 0:
            for line in coverage_result.stdout.split('\n'):
                if 'TOTAL' in line:
                    try:
                        coverage_percent = int(line.split()[-1].replace('%', ''))
                    except:
                        coverage_percent = 0
                    break
        
        coverage_score = min(coverage_percent, 50)  # æœ€å¤§50ç‚¹
        total_score = pytest_score + coverage_score
        
        return {
            'passed': total_score >= 80 and coverage_percent >= 95,
            'score': total_score,
            'details': {
                'pytest': f"{'âœ…' if pytest_passed else 'âŒ'} pytest ({pytest_score}/50)",
                'coverage': f"{'âœ…' if coverage_percent >= 95 else 'âŒ'} ã‚«ãƒãƒ¬ãƒƒã‚¸ {coverage_percent}% ({coverage_score}/50)",
                'coverage_percent': coverage_percent,
                'pytest_output': pytest_result.stdout + pytest_result.stderr,
                'coverage_output': coverage_result.stdout + coverage_result.stderr
            }
        }
    
    def _check_documentation(self) -> Dict:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯"""
        integrated_path = self.task_path / "integrated"
        docs_path = integrated_path / "docs"
        
        if not docs_path.exists():
            return {
                'passed': False,
                'score': 0,
                'details': 'docs ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“'
            }
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        doc_files = list(docs_path.rglob("*.md"))
        readme_exists = any("readme" in f.name.lower() for f in doc_files)
        api_docs_exist = any("api" in f.name.lower() for f in doc_files)
        
        file_count_score = min(len(doc_files) * 10, 40)
        readme_score = 30 if readme_exists else 0
        api_score = 30 if api_docs_exist else 0
        
        total_score = file_count_score + readme_score + api_score
        
        return {
            'passed': total_score >= 70,
            'score': total_score,
            'details': {
                'file_count': f"{'âœ…' if len(doc_files) >= 3 else 'âŒ'} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ•° {len(doc_files)} ({file_count_score}/40)",
                'readme': f"{'âœ…' if readme_exists else 'âŒ'} READMEæ–‡æ›¸ ({readme_score}/30)",
                'api_docs': f"{'âœ…' if api_docs_exist else 'âŒ'} APIæ–‡æ›¸ ({api_score}/30)",
                'doc_files': [str(f) for f in doc_files]
            }
        }
    
    def _check_performance(self) -> Dict:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯"""
        integrated_path = self.task_path / "integrated"
        
        # åŸºæœ¬çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
        if not (integrated_path / "src").exists():
            return {
                'passed': False,
                'score': 0,
                'details': 'src ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“'
            }
        
        # ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ™‚é–“æ¸¬å®š
        start_time = datetime.now()
        try:
            # ç°¡å˜ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ
            result = subprocess.run([
                sys.executable, "-c", 
                "import time; start=time.time(); import os; print(f'Import time: {time.time()-start:.3f}s')"
            ], capture_output=True, text=True, cwd=integrated_path)
            import_time = 0.1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            if result.returncode == 0:
                for line in result.stdout.split('\n'):
                    if 'Import time:' in line:
                        try:
                            import_time = float(line.split(':')[1].replace('s', '').strip())
                        except:
                            pass
        except:
            import_time = 1.0
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢ï¼ˆ1ç§’ä»¥ä¸‹ã§æº€ç‚¹ï¼‰
        performance_score = max(0, 100 - int(import_time * 100))
        
        return {
            'passed': performance_score >= 70,
            'score': performance_score,
            'details': {
                'import_time': f"{'âœ…' if import_time < 1.0 else 'âŒ'} ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ™‚é–“ {import_time:.3f}s ({performance_score}/100)",
                'import_time_value': import_time
            }
        }
    
    def _calculate_overall_score(self, results: Dict) -> float:
        """ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—"""
        weights = {
            'functional_test': 0.30,
            'code_quality': 0.25,
            'test_coverage': 0.25,
            'documentation': 0.15,
            'performance': 0.05
        }
        
        total_score = 0
        for category, result in results.items():
            weight = weights.get(category, 0)
            score = result.get('score', 0)
            total_score += score * weight
        
        return round(total_score, 2)
    
    def _make_integration_judgment(self, score: float, results: Dict) -> str:
        """çµ±åˆåˆ¤å®š"""
        critical_failures = []
        
        # é‡è¦ãªé …ç›®ã®ç¢ºèª
        if not results['functional_test']['passed']:
            critical_failures.append('æ©Ÿèƒ½è¦ä»¶æœªé”')
        
        if results['test_coverage']['details']['coverage_percent'] < 95:
            critical_failures.append('ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ä¸è¶³')
        
        if score >= 90 and len(critical_failures) == 0:
            return "INTEGRATE"  # ãã®ã¾ã¾çµ±åˆ
        elif score >= 70 and len(critical_failures) <= 1:
            return "MINOR_FIX"  # è»½å¾®ä¿®æ­£å¾Œçµ±åˆ
        else:
            return "MAJOR_REWORK"  # å¤§å¹…ä¿®æ­£ãƒ»å†ä½œæˆ
    
    def _get_recommended_action(self, judgment: str) -> str:
        """æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å–å¾—"""
        actions = {
            "INTEGRATE": "é«˜å“è³ªå®Œæˆã€‚ãã®ã¾ã¾çµ±åˆã—ã¦æ¬¡ã®ã‚¿ã‚¹ã‚¯ã«é€²ã‚€ã€‚",
            "MINOR_FIX": "è»½å¾®ãªä¿®æ­£ã‚’é©ç”¨å¾Œã€çµ±åˆã™ã‚‹ã€‚",
            "MAJOR_REWORK": "å“è³ªåŸºæº–æœªé”ã€‚Boss ã«è©³ç´°ãªæ”¹å–„æŒ‡ç¤ºã‚’é€ã‚Šã€å†ä½œæˆã‚’æ±‚ã‚ã‚‹ã€‚"
        }
        return actions.get(judgment, "åˆ¤å®šã‚¨ãƒ©ãƒ¼")
    
    def _extract_issues(self, results: Dict) -> List[str]:
        """å•é¡Œç‚¹æŠ½å‡º"""
        issues = []
        
        for category, result in results.items():
            if not result['passed']:
                if category == 'functional_test':
                    issues.append("æ©Ÿèƒ½è¦ä»¶ãŒæº€ãŸã•ã‚Œã¦ã„ã¾ã›ã‚“")
                elif category == 'code_quality':
                    issues.append("ã‚³ãƒ¼ãƒ‰å“è³ªåŸºæº–ã‚’æº€ãŸã—ã¦ã„ã¾ã›ã‚“")
                elif category == 'test_coverage':
                    issues.append("ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ãŒ95%æœªæº€ã§ã™")
                elif category == 'documentation':
                    issues.append("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒä¸ååˆ†ã§ã™")
                elif category == 'performance':
                    issues.append("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã¾ã›ã‚“")
        
        return issues
    
    def _generate_fix_suggestions(self, results: Dict) -> List[str]:
        """ä¿®æ­£ææ¡ˆç”Ÿæˆ"""
        suggestions = []
        
        # ã‚³ãƒ¼ãƒ‰å“è³ªã®ä¿®æ­£ææ¡ˆ
        if not results['code_quality']['passed']:
            details = results['code_quality']['details']
            if 'flake8' in details and 'âŒ' in details['flake8']:
                suggestions.append("flake8 ã®è­¦å‘Šã‚’ä¿®æ­£ã—ã¦ãã ã•ã„")
            if 'black' in details and 'âŒ' in details['black']:
                suggestions.append("black ã§ã‚³ãƒ¼ãƒ‰ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã¦ãã ã•ã„")
        
        # ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ã®ä¿®æ­£ææ¡ˆ
        if not results['test_coverage']['passed']:
            coverage = results['test_coverage']['details']['coverage_percent']
            if coverage < 95:
                suggestions.append(f"ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’95%ä»¥ä¸Šã«ã—ã¦ãã ã•ã„ï¼ˆç¾åœ¨{coverage}%ï¼‰")
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä¿®æ­£ææ¡ˆ
        if not results['documentation']['passed']:
            details = results['documentation']['details']
            if 'âŒ' in details.get('readme', ''):
                suggestions.append("README.md ã‚’ä½œæˆã—ã¦ãã ã•ã„")
            if 'âŒ' in details.get('api_docs', ''):
                suggestions.append("API ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„")
        
        return suggestions
    
    def _save_evaluation_report(self, report: Dict):
        """è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜"""
        report_path = self.task_path / f"evaluation_{self.org_name}_{self.task_name}.json"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")

def main():
    parser = argparse.ArgumentParser(description='Task Quality Evaluator')
    parser.add_argument('org_name', help='Organization name (e.g., org-01)')
    parser.add_argument('task_name', help='Task name (e.g., database_module)')
    parser.add_argument('--output', help='Output format', choices=['json', 'text'], default='text')
    
    args = parser.parse_args()
    
    try:
        evaluator = TaskQualityEvaluator(args.org_name, args.task_name)
        result = evaluator.evaluate_completion()
        
        if args.output == 'json':
            print(json.dumps(result, ensure_ascii=False, indent=2))
        else:
            # ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ã®å‡ºåŠ›
            print(f"\nğŸ† {args.org_name} {args.task_name} å“è³ªè©•ä¾¡çµæœ")
            print("=" * 50)
            print(f"ğŸ“Š ç·åˆã‚¹ã‚³ã‚¢: {result['overall_score']}/100")
            print(f"ğŸ¯ åˆ¤å®š: {result['judgment']}")
            print(f"ğŸ’¡ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {result['recommended_action']}")
            
            if result['issues']:
                print(f"\nğŸš¨ ä¸»è¦ãªå•é¡Œ:")
                for issue in result['issues']:
                    print(f"  - {issue}")
            
            if result['fix_suggestions']:
                print(f"\nğŸ”§ ä¿®æ­£ææ¡ˆ:")
                for suggestion in result['fix_suggestions']:
                    print(f"  - {suggestion}")
            
            print(f"\nğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: orgs/{args.org_name}/{args.task_name}/evaluation_{args.org_name}_{args.task_name}.json")
        
        # çµ‚äº†ã‚³ãƒ¼ãƒ‰è¨­å®š
        if result['judgment'] == 'INTEGRATE':
            sys.exit(0)
        elif result['judgment'] == 'MINOR_FIX':
            sys.exit(1)
        else:  # MAJOR_REWORK
            sys.exit(2)
            
    except Exception as e:
        print(f"âŒ è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(3)

if __name__ == "__main__":
    main() 